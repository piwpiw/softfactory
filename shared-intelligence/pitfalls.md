# üìù Shared Intelligence ‚Äî Pitfalls Registry

> **Purpose**: - **Date:** 2026-02-23
> **Status**: üü¢ ACTIVE (Í¥ÄÎ¶¨ Ï§ë)
> **Impact**: [Engineering / Operations]

---

## ‚ö° Executive Summary (ÌïµÏã¨ ÏöîÏïΩ)
- **Ï£ºÏöî ÎÇ¥Ïö©**: Î≥∏ Î¨∏ÏÑúÎäî Shared Intelligence ‚Äî Pitfalls Registry Í¥ÄÎ†® ÌïµÏã¨ Î™ÖÏÑ∏ Î∞è Í¥ÄÎ¶¨ Ìè¨Ïù∏Ìä∏Î•º Ìè¨Ìï®Ìï©ÎãàÎã§.
- **ÏÉÅÌÉú**: ÌòÑÏû¨ ÏµúÏã†Ìôî ÏôÑÎ£å Î∞è Í≤ÄÌÜ† Îê®.
- **Ïó∞Í¥Ä Î¨∏ÏÑú**: [Master Index](./NOTION_MASTER_INDEX.md)

---

> **Purpose:** Every task appends here ‚Äî zero silent failures, zero repeated mistakes.
> **Format:** Date | Agent | Task | Pitfall | Prevention
> **Requirement:** Mandatory update per Governance Principle #9

---

## Python / Backend

### PF-001: Python Decorator Execution Order
- **Date:** 2026-02-23
- **Agent:** 05-Backend
- **Task:** M-003 SoftFactory API routes
- **Pitfall:** Decorators execute bottom-to-top. Writing `@require_subscription('x') @require_auth` causes `@require_auth` to run first ‚Äî but `@require_subscription` was checking `user_id` before auth ran, causing `AttributeError`.
- **Prevention:** Always place `@require_auth` on the innermost (bottom) position. Rule: auth decorators go LAST in source order.
- **Files Fixed:** `backend/services/coocook.py`, `sns_auto.py`, `review.py`, `ai_automation.py` (17 routes total)

### PF-002: SQLite Relative Path Fails on Process Restart
- **Date:** 2026-02-23
- **Agent:** 05-Backend
- **Task:** M-003 DB initialization
- **Pitfall:** `sqlite:///platform.db` resolves relative to CWD ‚Äî if Flask is started from a different directory, a second database file is created silently.
- **Prevention:** Always use absolute path: `sqlite:///D:/Project/platform.db`. Set via `os.path.abspath()` in `app.py`.

### PF-003: Demo Token Format Inconsistency
- **Date:** 2026-02-24
- **Agent:** 05-Backend
- **Task:** M-003 API testing
- **Pitfall:** Demo token was generated as `'demo_token_' + timestamp` but `@require_auth` checked for static string `'demo_token'`. Caused 401 on all demo requests.
- **Prevention:** Standardize demo token as static string `'demo_token'`. Never append dynamic suffixes to static-check tokens.

### PF-004: Missing `to_dict()` on SQLAlchemy Models
- **Date:** 2026-02-24
- **Agent:** 05-Backend
- **Task:** M-003 AI Automation endpoints
- **Pitfall:** `AIEmployee` and `Scenario` models lacked `to_dict()` ‚Äî returning raw SQLAlchemy objects caused `TypeError: Object of type AIEmployee is not JSON serializable`.
- **Prevention:** Every SQLAlchemy model MUST implement `to_dict()`. Add to model template and code review checklist.

---

## Infrastructure / DevOps

### PF-005: Virtual Environment Path Breaks Sonolbot
- **Date:** 2026-02-23
- **Agent:** 09-DevOps
- **Task:** M-005 Sonolbot daemon setup
- **Pitfall:** Sonolbot daemon fails silently with "missing venv python runtime" if `.venv/Scripts/pythonw.exe` is absent or path is wrong.
- **Prevention:** Always verify `daemon/.venv/Scripts/pythonw.exe` exists before starting daemon. Control panel checks on launch.

---

## Documentation

### PF-006: MEMORY.md Line Limit (200 lines)
- **Date:** 2026-02-25
- **Agent:** Orchestrator
- **Task:** Session memory management
- **Pitfall:** `MEMORY.md` grew to 365 lines ‚Äî only first 200 loaded into context. Critical project info was being silently truncated.
- **Prevention:** Keep MEMORY.md ‚â§200 lines. Extract detail into topic files (e.g., `memory/project-status.md`). MEMORY.md is an index, not a dump.

### PF-007: GitHub Workflows Missing Health Check Gate
- **Date:** 2026-02-25
- **Agent:** 09-DevOps
- **Task:** #6 CI/CD hardening + monitoring
- **Pitfall:** Deploy workflow lacked health check validation before marking deployment as successful. Broken deployments appeared as "green" for 5+ minutes.
- **Prevention:** Add health check gate to all deploy workflows. Retry logic on 502/503. Hard fail on 5 consecutive failures. Check endpoint before and after deployment.

### PF-008: Prometheus Config Not Validated Before Deploy
- **Date:** 2026-02-25
- **Agent:** 09-DevOps
- **Task:** #6 Infrastructure monitoring setup
- **Pitfall:** Invalid Prometheus YAML syntax silently fails ‚Äî metrics collection appears off but no error alert fires.
- **Prevention:** Add `promtool check config` to lint pipeline. Validate all YAML before commit. Add to CI/CD.

### PF-009: Pre-commit Hooks Don't Execute in CI
- **Date:** 2026-02-25
- **Agent:** 09-DevOps
- **Task:** #6 Pre-commit hook setup
- **Pitfall:** Pre-commit hooks work locally but skip in GitHub Actions CI. Allows quality standards bypass on automated commits.
- **Prevention:** Replicate pre-commit checks in GitHub Actions workflow. Never trust local hooks for quality gate. Use workflow as source of truth.

### PF-007: Missing Scheduler Shutdown on Daemon Exit
- **Date:** 2026-02-25
- **Agent:** 09-DevOps / Sonolbot Enhancement
- **Task:** M-005 Sonolbot v2.0 ‚Äî APScheduler integration
- **Pitfall:** BackgroundScheduler started in `_init_scheduler()` but never shut down in `run()` finally block. Result: daemon process hangs on Ctrl+C, hanging jobs block OS shutdown.
- **Prevention:** Always call `scheduler.shutdown()` before releasing daemon lock. Add to finally block: `if self.scheduler and self.scheduler.running: self.scheduler.shutdown()`
- **Files Fixed:** `daemon/daemon_service.py` (line ~430)

### PF-008: Command Logging Without Directory Validation
- **Date:** 2026-02-25
- **Agent:** 09-DevOps / Sonolbot Enhancement
- **Task:** M-005 Sonolbot v2.0 ‚Äî Command audit logging
- **Pitfall:** `_log_command()` appends to `command_history.log` without checking if parent directory exists. First command write fails silently.
- **Prevention:** Create parent directory with `mkdir(parents=True, exist_ok=True)` before writing. Always wrap in try/except to avoid crashing message handler.
- **Files Fixed:** `daemon/daemon_service.py` method `_log_command()` (line ~870)

### PF-009: GET Endpoints Exposing Data Without Auth
- **Date:** 2026-02-25
- **Agent:** QA Engineer (Phase 3)
- **Task:** M-002 CooCook QA ‚Äî API security review
- **Pitfall:** `GET /api/coocook/chefs` and `GET /api/coocook/chefs/{id}` lack `@require_auth` decorator. While intentional for public discovery, this is a design decision that must be documented. Risk: prevents future permission changes.
- **Prevention:** Document public endpoints explicitly in API spec. Mark clearly: "This endpoint intentionally public for user discovery." For protected variants, use separate endpoints (e.g., `/api/coocook/chefs-premium`).
- **Status:** ‚úÖ By Design (not a bug, but document it)
- **Reference:** M-002 Phase 3 QA Report, Security section

### PF-010: Response Time Benchmarking Must Include Multiple Runs
- **Date:** 2026-02-25
- **Agent:** QA Engineer (Phase 3)
- **Task:** M-002 CooCook QA ‚Äî Performance validation
- **Pitfall:** Single API response time test can be anomalous due to caching, system load, etc. 220ms on first run, 180ms on third run due to warm cache.
- **Prevention:** Always measure endpoints 3+ times and report average. Report min/max range. Useful for identifying cache behavior.
- **Example:** GET /chefs: 214ms, 221ms, 218ms ‚Üí Average 218ms (variance ¬±3ms indicates stable performance)

### PF-011: JSON Path Extraction with grep Fragile on Complex Responses
- **Date:** 2026-02-25
- **Agent:** QA Engineer (Phase 3)
- **Task:** M-002 CooCook QA ‚Äî Test automation
- **Pitfall:** Using `curl | grep '"id":[0-9]*'` to extract booking ID from JSON response. Works for simple responses but fails when response includes multiple objects. Returns multiple matches, making ID extraction unreliable.
- **Prevention:** Use proper JSON parser (jq, python -m json.tool) when available. If scripting QA tests, prefer API client libraries over shell curl + grep.
- **Workaround:** For bash, use Python: `python3 -c "import json, sys; print(json.load(sys.stdin)['id'])"`

---

## Docker / Migration

### PF-012: Docker Desktop Daemon Check Must Happen Before Compose Commands
- **Date:** 2026-02-25
- **Agent:** DevOps (M-002 Phase 4)
- **Task:** PostgreSQL Docker initialization
- **Pitfall:** Running `docker-compose up -d db` when Docker Desktop is not running silently fails with cryptic error: `Cannot connect to Docker daemon at npipe:////./pipe/dockerDesktopLinuxEngine`. User thinks compose syntax is wrong, when Docker daemon is actually offline.
- **Prevention:** **Always verify Docker daemon first:** `docker ps` (on Windows, this launches Docker Desktop if needed, but check output). If error mentions "pipe" or "daemon", start Docker Desktop GUI before retrying.
- **Workaround:** Windows GUI: Start ‚Üí "Docker Desktop" ‚Üí wait 60 seconds for taskbar icon to stabilize ‚Üí retry compose commands.

### PF-013: PostgreSQL Container Initialization Time Not Awaited
- **Date:** 2026-02-25
- **Agent:** DevOps (M-002 Phase 4)
- **Task:** SQLite ‚Üí PostgreSQL migration setup
- **Pitfall:** Running `docker-compose up -d db` followed immediately by `python scripts/migrate_to_postgres.py` fails with `Connection refused on port 5432`. PostgreSQL container is running, but `initdb` not yet complete (takes 10-15 seconds).
- **Prevention:** After `docker-compose up -d db`, wait 15+ seconds OR loop-check: `docker logs project_db_1 | grep "ready to accept connections"`. Migration script should include retry logic with exponential backoff.
- **Fixed in:** `scripts/migrate_to_postgres.py` v1.1 (adds 3 retry attempts with 5s delays)

### PF-014: SQLite Source DB Must Be Closed Before Migration
- **Date:** 2026-02-25
- **Agent:** DevOps (M-002 Phase 4)
- **Task:** SQLite ‚Üí PostgreSQL migration
- **Pitfall:** If Flask or other process has `platform.db` file open, migration script cannot read it cleanly. Result: partial migration or "database is locked" error.
- **Prevention:** Ensure Flask is stopped: `docker-compose down` (if running). Check no other processes: `lsof | grep platform.db` (Linux/Mac) or Task Manager search `platform.db` (Windows). Then start fresh: `docker-compose up -d db && python scripts/migrate_to_postgres.py`.

### PF-015: PostgreSQL Volume Persists After Container Removal
- **Date:** 2026-02-25
- **Agent:** DevOps (M-002 Phase 4)
- **Task:** Docker cleanup after failed tests
- **Pitfall:** Running `docker-compose down` removes containers but NOT volumes. If you want a fresh database, the old `postgres_data` volume still exists, and `docker-compose up` reattaches it (with old data). User expects "fresh start" but gets stale database.
- **Prevention:** To fully reset: `docker-compose down -v` (removes volumes). To selectively remove: `docker volume rm project_postgres_data`. Always verify with `docker volume ls` before restart.

---

## Documentation / Deployment

### PF-016: Deployment Checklist Must Cover Docker Desktop Detection
- **Date:** 2026-02-25
- **Agent:** DevOps (M-002 Phase 4)
- **Task:** DEPLOYMENT_CHECKLIST.md creation
- **Pitfall:** Initial checklist assumed Docker Desktop was running. First time following it, user was confused why `docker ps` fails. Doesn't teach WHERE to start Docker.
- **Prevention:** Include explicit "Check Docker Desktop is Running" as Step 0 in all deployment guides. Provide screenshots or CLI commands to verify daemon status.
- **File Updated:** `DEPLOYMENT_CHECKLIST.md` Part 2 (includes GUI + CLI startup methods)

### PF-012: Database Verification Without Query Tool Requires Python Fallback
- **Date:** 2026-02-25
- **Agent:** QA Engineer (Phase 3)
- **Task:** M-002 CooCook QA ‚Äî Data integrity check
- **Pitfall:** Attempted `sqlite3` CLI tool for database validation but CLI not in PATH on test system. Had to fall back to Python `sqlite3` library.
- **Prevention:** In QA scripts, prefer Python scripting over CLI tools for database verification. More portable across environments.
- **Script Pattern:**
  ```python
  import sqlite3
  conn = sqlite3.connect("D:/Project/platform.db")
  cursor = conn.cursor()
  cursor.execute("SELECT COUNT(*) FROM booking WHERE total_price > 0")
  print(f"Valid bookings: {cursor.fetchone()[0]}")
  ```

### PF-021: UI Components Missing Accessibility Hints (A11y)
- **Date:** 2026-02-25
- **Agent:** UI/UX Enhancement
- **Task:** M-003 SoftFactory UI improvement Phase 1
- **Pitfall:** Custom code editor textarea lacked proper ARIA labels. Template marketplace search wasn't keyboard navigable. Analytics dashboard charts had no alt text for screen readers.
- **Prevention:** Every interactive component MUST have: (1) semantic HTML (button, input, select), (2) ARIA labels for complex widgets, (3) keyboard navigation support, (4) focus indicators visible. Audit with axe DevTools before deployment.
- **Files Fixed:** `web/ai-automation/code.html`, `web/marketplace/index.html`, `web/analytics/dashboard.html`

### PF-022: Responsive Design Breaks on Mobile < 375px
- **Date:** 2026-02-25
- **Agent:** UI/UX Enhancement
- **Task:** M-003 SoftFactory UI improvement Phase 1
- **Pitfall:** New marketplace grid uses `md:grid-cols-2` min-width 768px. On iPhone 6 (375px width), cards overflow. Text in code editor becomes unreadable on small phones.
- **Prevention:** Test on 320px, 375px, 425px viewports. Use `grid-cols-1` for mobile-first. Set `max-width: 100vw` to prevent horizontal scroll. Test with `viewport-width: device-width` meta tag.
- **Testing:** `throttle_device: 'iPhone SE'` in E2E tests

### PF-023: Color Contrast on Analytics Dashboard Metrics
- **Date:** 2026-02-25
- **Agent:** UI/UX Enhancement
- **Task:** M-003 SoftFactory UI improvement Phase 1
- **Pitfall:** Metric cards use CSS variables for gradient colors (`var(--color-from)`). Some gradient combos (purple-600 to purple-700) fail WCAG AA contrast ratio (3.8:1 instead of 4.5:1 required).
- **Prevention:** Pre-compute all color combinations in dev. Run through axe or WAVE contrast checker. For gradients, use darker `to` color or lighter `from` color. Target ‚â•4.5:1 ratio for all text.
- **Reference:** `web/analytics/dashboard.html` ‚Äî all metric card gradients now WCAG AA compliant

---

### PF-007: Inconsistent Crawler Output Format
- **Date:** 2026-02-25
- **Agent:** 03-Architect, 05-Backend
- **Task:** M-006 Experience Platform crawlers
- **Pitfall:** Different crawlers return different field names/types (e.g., `expiration` vs `deadline`, datetime vs string). Service layer expects consistent format; inconsistent crawlers cause JSON serialization errors.
- **Prevention:** Define `ExperienceCrawler.format_listings()` that normalizes all output to standard dict schema BEFORE saving. Validate with `validate_listing()` to ensure required keys exist.
- **Files Fixed:** `scripts/crawlers/crawler_base.py` (abstract validation method), `backend/services/experience.py` (schema documentation)

### PF-008: Mixing Dummy Data with Real Database Persistence
- **Date:** 2026-02-25
- **Agent:** 05-Backend
- **Task:** M-006 Experience Platform MVP
- **Pitfall:** If dummy data is embedded in service layer AND database layer saves to DB, later code cannot distinguish real data from test data. Causes "ghost listings" that never disappear.
- **Prevention:** For MVP phase, keep all dummy data in service layer ONLY. Use decorator or environment variable to toggle: `if ENABLE_REAL_CRAWLERS: use_crawlers() else: use_dummy_data()`. Database layer is clean and ready for Phase 5.
- **Files Fixed:** `backend/services/experience.py` (DUMMY_LISTINGS constant isolated)

### PF-009: Silent Crawler Failures Without Logging
- **Date:** 2026-02-25
- **Agent:** 05-Backend, 06-QA
- **Task:** M-006 crawler framework
- **Pitfall:** Crawler fails to fetch page (timeout, 403, network error) but service returns empty list without logging. QA sees "no listings found" and debugs wrong area (service layer) instead of crawler.
- **Prevention:** Every crawler must log errors to `CrawlerLog` table with status='error' and error_message. Service layer MUST check log before returning data. Crawler failures visible in `/api/experience/stats` response.
- **Files Fixed:** `scripts/crawlers/crawler_base.py` (try/except with CrawlerLog), `backend/models.py` (CrawlerLog model)

### PF-017: Missing Correlation IDs in Logs Makes Debugging Hard
- **Date:** 2026-02-25
- **Agent:** 04-DevOps / Monitoring
- **Task:** M-004 Production Monitoring Setup
- **Pitfall:** When an error occurs, logs for that request are scattered across many lines. Without a correlation ID (request_id), tracing a single user's request through database queries, API calls, and error handlers requires manual log grepping across timestamps. Result: 30-minute debugging that should take 5 minutes.
- **Prevention:** Every Flask request must generate a unique request_id (UUID or incrementing counter) in `@app.before_request`. Inject into all log entries. Include in response headers (X-Request-Id). When debugging, grep logs for that single request_id to see full request lifecycle.
- **Reference:** `backend/logging_config.py` ‚Äî RequestIdFilter, request_logging_middleware functions
- **Files Fixed:** `backend/logging_config.py` (new), `backend/metrics.py` (new)

### PF-018: Prometheus Scrape Failures Silent When Metrics Endpoint Not Added
- **Date:** 2026-02-25
- **Agent:** 04-DevOps / Monitoring
- **Task:** M-004 Production Monitoring Setup
- **Pitfall:** Flask app running, all endpoints work, but Prometheus shows target as "DOWN" ‚Äî seems to indicate service crash. Actually, the `/api/metrics/prometheus` endpoint was never registered. Prometheus retries silently every 15 seconds.
- **Prevention:** Always verify metrics endpoints exist before starting Prometheus scrape job. Test endpoint manually: `curl http://localhost:8000/api/metrics/prometheus`. If 404, check that `app.register_blueprint(metrics_bp)` is called in create_app().
- **Reference:** `backend/metrics.py` ‚Äî Blueprint registration required
- **Status:** Caught in Step 6 of MONITORING-INTEGRATION.md

### PF-019: JSON Logs Without Proper Escaping Cause Kibana Parse Failures
- **Date:** 2026-02-25
- **Agent:** 04-DevOps / Monitoring
- **Task:** M-004 Production Monitoring Setup
- **Pitfall:** Application logs messages that contain newlines or quotes without escaping. Logstash attempts to parse as JSON but fails. Result: unparseable log entries in Elasticsearch, Kibana shows zero results.
- **Prevention:** Use python-json-logger package which auto-escapes. Never manually construct JSON strings in logs. Always use logger.info('message', extra={'key': 'value'}) pattern. Test: `echo "message with \"quotes\"" | python -m json.tool`
- **Reference:** `backend/logging_config.py` uses JSONFormatter from python-json-logger
- **Testing:** Run: `tail logs/app.log | python -m json.tool` ‚Äî should parse without errors

### PF-020: Alert Rules With Wrong Metric Names Silent Fail
- **Date:** 2026-02-25
- **Agent:** 04-DevOps / Monitoring
- **Task:** M-004 Production Monitoring Setup
- **Pitfall:** Write alert rule like `expr: high_memory > 80` but metric is actually named `softfactory_memory_percent`. Prometheus silently evaluates to "no data" and never fires alert. No error in logs, no indication why alert not triggering.
- **Prevention:** Before adding alert rule, verify metric exists: `curl http://localhost:8000/api/metrics/prometheus | grep metric_name`. Test PromQL query in Prometheus UI (localhost:9090/graph) before adding to alert-rules.yml.
- **Reference:** `orchestrator/alert-rules.yml` ‚Äî all metric names verified against actual exports
- **Testing:** Script in MONITORING-SETUP.md validates all rule metrics

---

## Template for New Entries

```markdown
### PF-XXX: [Short Title]
- **Date:** YYYY-MM-DD
- **Agent:** [agent-id]
- **Task:** [project/task]
- **Pitfall:** [what went wrong and why]
- **Prevention:** [concrete rule to prevent recurrence]
- **Files Fixed:** [if applicable]
```

## PF-008: Database Query N+1 Pattern (2026-02-25)
**Severity:** HIGH | **Impact:** 40-60% performance degradation | **Detection:** Automated tests

### Problem
Multiple endpoints execute N+1 queries instead of single JOIN queries:
- Campaign listing: 1 query + 12 COUNT queries (13 total, 42ms)
- Dashboard stats: 6 separate COUNT queries (58ms)
- SNS account counts: 1 query + 5 COUNT queries (25ms)

### Root Cause
Using nested loops with database queries:
```python
for campaign in campaigns:
    count = CampaignApplication.query.filter_by(campaign_id=campaign.id).count()
```

### Prevention (Now Standard)
1. Always use JOINs with GROUP BY for aggregations
2. Use `joinedload()` for eager loading relationships
3. Batch COUNT operations with single query
4. Automated tests verify query count == 1

### Reference
- Report: `docs/database-optimization-report.md`
- Quick fix: `docs/DATABASE_OPTIMIZATION_QUICKSTART.md`
- Examples: `backend/query_optimization_examples.py`
- Tests: `tests/test_database_performance.py`


### PF-021: Gunicorn Worker Starvation Without Proper Configuration
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** Dockerfile.prod creation for production workloads
- **Pitfall:** Default Gunicorn configuration uses 1 worker, causing request queuing and high latency under load. Deployment with default settings causes P95 response times > 5 seconds.
- **Prevention:** Always set `--workers` to at least 2 √ó CPU cores. For 2-core instance: 4 workers. Formula: `workers = (2 √ó cpu_count) + 1`. Set in docker-compose-prod.yml: `WORKERS=4` or pass via docker-entrypoint.
- **Reference:** `Dockerfile.prod` line 52 uses 4 workers; `docker-compose-prod.yml` WORKERS env var

### PF-022: Docker Multi-stage Build Layer Cache Invalidation
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** Optimize Dockerfile.prod image size
- **Pitfall:** Copying requirements.txt after COPY . . causes entire app layer to rebuild on any code change. Results in 5-10 minute builds when only Python deps should cache.
- **Prevention:** Always order Dockerfile commands by change frequency: rarely-changing (base, system deps) ‚Üí sometimes-changing (Python deps) ‚Üí frequently-changing (app code). Copy requirements.txt in builder, then copy wheels in runtime.
- **Reference:** `Dockerfile.prod` stage structure ‚Äî builder copies requirements, runtime copies wheels

### PF-023: PostgreSQL InitDB Waits Not Respected in docker-compose
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** docker-compose-prod.yml creation with db health checks
- **Pitfall:** Setting `depends_on: condition: service_healthy` does not guarantee DB is fully initialized. `pg_isready` returns true before `initdb` completes. Migrations fail with "database does not exist" or "relation does not exist".
- **Prevention:** After `docker-compose up -d db`, always wait 15+ seconds. Use: `sleep 20` or loop with `docker logs` check for "ready to accept connections". Migrations should include retry logic with exponential backoff (3 attempts, 5s delays).
- **Reference:** `scripts/deploy.sh` Phase 6 includes `sleep 30` before migrations

### PF-024: Nginx SSL Certificate Path Errors in Docker Context
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** nginx/nginx.conf SSL configuration
- **Pitfall:** nginx container fails to start if `/etc/nginx/ssl/cert.pem` or `/etc/nginx/ssl/key.pem` missing. Error message is cryptic: "open() failed (2: No such file or directory)". Result: reverse proxy down, all traffic fails.
- **Prevention:** ALWAYS create dummy self-signed certificates before first docker-compose up. OR mount only if exists (use conditional in docker-compose). OR use non-SSL nginx config for development.
- **Quick fix:** `openssl req -x509 -newkey rsa:2048 -nodes -keyout nginx/ssl/key.pem -out nginx/ssl/cert.pem -days 365` before docker-compose up

### PF-025: Docker Compose Environment Variable Scope Issues
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** .env-prod integration with docker-compose-prod.yml
- **Pitfall:** Variables in .env-prod file are NOT automatically available inside containers. `docker-compose up` reads .env for Compose interpolation ONLY. Services receive environment vars only if explicitly listed in `environment:` section or via `--env-file` flag.
- **Prevention:** Always use `docker-compose --env-file .env-prod up` OR reference variables explicitly in docker-compose.yml `environment:` section. Do NOT assume .env variables magically appear in container.
- **Reference:** `docker-compose-prod.yml` line 35: `environment:` section explicitly lists all vars from .env-prod

### PF-026: Rate Limiting Configuration Without Burst Allowance
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** nginx/nginx.conf rate limiting setup
- **Pitfall:** Setting `limit_req zone=api_limit burst=0` causes EVERY request to be throttled strictly. Legitimate traffic spikes (batch jobs, mobile app syncs) result in 429 Too Many Requests. Real-world: 50 concurrent users ‚Üí 30% get throttled even under 100r/s limit.
- **Prevention:** Always set `burst > max_requests_per_second`. For 100r/s limit: set `burst=200`. This allows short spikes while maintaining rate ceiling. Use `nodelay` for immediate rejection OR remove for queue processing.
- **Reference:** `nginx/nginx.conf` line 79: `burst=200` allows spike absorption

### PF-027: Database Connection Pool Exhaustion Without Limits
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** docker-compose-prod.yml PostgreSQL pooling
- **Pitfall:** Flask app creates new DB connection per request without pooling. With 4 Gunicorn workers √ó 50 requests/sec = 200 connections. PostgreSQL default `max_connections=100` ‚Üí "too many connections" errors.
- **Prevention:** (1) Increase `max_connections` in PostgreSQL: `POSTGRES_INITDB_ARGS` in docker-compose. (2) Better: Add PgBouncer or SQLAlchemy pooling in Flask. (3) Set connection timeouts: `connection_idle_time=300`.
- **Reference:** `docker-compose-prod.yml` line 77: `max_connections=100` tuned for 4 workers

### PF-028: Backup Retention Policy Never Triggered Without Cron
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** scripts/backup.sh creation
- **Pitfall:** Wrote comprehensive backup script with 30-day retention cleanup, but script only runs once manually. Month later: 30GB of backups eating all disk space. Retention logic ignored because cleanup only runs if script invoked.
- **Prevention:** ALWAYS set up cron job immediately after backup script deployment. Document exact cron line in README. Example: `0 2 * * * /path/to/backup.sh >> /var/log/softfactory-backup.log 2>&1`. Verify: `crontab -l` OR `systemctl list-timers`.
- **Reference:** `scripts/backup.sh` line 1 includes cron example; must be manually added to system

### PF-029: Health Check Timeouts Cause Cascading Failures
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** docker-compose-prod.yml healthcheck configuration
- **Pitfall:** Health check interval 10s with timeout 5s means if any check takes >5s, container marked unhealthy. Under load, slow DB query causes health fail ‚Üí Docker restarts container ‚Üí connection reset ‚Üí cascade restart loop.
- **Prevention:** Set timeout generously (10-15s). Set interval to longer period (30s) for production. Add `start_period` buffer (30s) to allow app warmup before health checks start. Max retries should be 3-5 (not 1).
- **Reference:** `docker-compose-prod.yml` line 43: `start-period: 5s` provides warmup buffer

### PF-030: Secrets in Docker Logs Compromise Security
- **Date:** 2026-02-25
- **Agent:** DevOps Engineer (Production Deployment)
- **Task:** Production security review
- **Pitfall:** Flask app logs DATABASE_URL which contains password in plaintext. Attacker with `docker logs` access gets credentials. Same issue: Redis password, JWT secret visible in startup logs.
- **Prevention:** (1) Never log sensitive values. (2) Use Docker secrets (Swarm) or mounted secret files (Kubernetes). (3) Log only redacted versions: "DB=psql://***@host" (4) Enable log rotation immediately: `max-size: 10m` in docker-compose.
- **Reference:** `docker-compose-prod.yml` logging configs include size limits; Flask app should not log DATABASE_URL

---

## Security & Authentication

### PF-041: Hardcoded Demo Token Bypasses Authentication
- **Date:** 2026-02-25
- **Agent:** Security Auditor
- **Task:** M-003 SoftFactory security audit
- **Pitfall:** `@require_auth` decorator accepted static string `'demo_token'` that completely bypassed JWT verification. Anyone knowing this magic string could access authenticated endpoints without valid credentials. CVSS 9.8 CRITICAL.
- **Prevention:** Remove ALL hardcoded token bypasses. All authentication must go through JWT verification layer. Every request must have `payload = verify_token(token)` check.
- **Files Fixed:** `backend/auth.py` lines 50-79 (require_auth decorator), lines 111-138 (require_subscription decorator)
- **Status:** ‚úÖ FIXED in v2.0

### PF-042: No Password Strength Requirements
- **Date:** 2026-02-25
- **Agent:** Security Auditor
- **Task:** M-003 SoftFactory security audit
- **Pitfall:** System accepted passwords as short as 1 character with no complexity. Users could set 'a' or 'password' as valid passwords. Brute force and dictionary attacks trivial. CVSS 8.6 HIGH.
- **Prevention:** Enforce minimum 8 chars, 1 uppercase, 1 digit, 1 special char. Reject common patterns (password, qwerty, 123456). Validate at both registration and password change endpoints.
- **Files Created:** `backend/password_validator.py` (validation engine)
- **Files Fixed:** `backend/auth.py` lines 142-165 in `/register` endpoint
- **Status:** ‚úÖ FIXED in v2.0

### PF-043: No Rate Limiting on Authentication Endpoints
- **Date:** 2026-02-25
- **Agent:** Security Auditor
- **Task:** M-003 SoftFactory security audit
- **Pitfall:** Login endpoint had no rate limiting. Attacker could attempt unlimited password guesses at any speed. No protection against brute force or credential stuffing. CVSS 7.5 HIGH.
- **Prevention:** Implement rate limiting: max 5 failed attempts per minute per email. Lock account after 5 failures for 15 minutes. Log all attempts with IP + timestamp. Use decorator `@require_rate_limit` on auth endpoints.
- **Files Created:** `backend/security_middleware.py` (rate limiting, lockout, audit logging)
- **Files Changed:** `backend/models.py` (added LoginAttempt table, security fields to User)
- **Files Changed:** `backend/auth.py` (integrated rate limiting into /login)
- **Status:** ‚úÖ FIXED in v2.0

### PF-044: No Account Lockout Mechanism
- **Date:** 2026-02-25
- **Agent:** Security Auditor
- **Task:** M-003 SoftFactory security audit
- **Pitfall:** Failed login attempts never triggered account lockout. Unlimited guesses possible. No protection for compromised accounts.
- **Prevention:** Lock account after 5 failed attempts. Auto-unlock after 15 minutes. Track in User model: `is_locked`, `locked_until` fields. Log lockout events with timestamp.
- **Files Created:** `backend/security_middleware.py` (LoginAttemptTracker class)
- **Files Changed:** `backend/models.py` (User.is_locked, User.locked_until fields)
- **Status:** ‚úÖ FIXED in v2.0

### PF-045: No Audit Logging for Security Events
- **Date:** 2026-02-25
- **Agent:** Security Auditor
- **Task:** M-003 SoftFactory security audit
- **Pitfall:** No logging of login attempts, failed authentications, or account lockouts. Impossible to detect or investigate attacks after the fact.
- **Prevention:** Log every security event: LOGIN_SUCCESS, LOGIN_FAILED, RATE_LIMIT_EXCEEDED, ACCOUNT_LOCKED, USER_REGISTERED, PASSWORD_CHANGED. Include timestamp, user email, IP address, details. Store in `logs/security_audit.log`.
- **Files Created:** `backend/security_middleware.py` (SecurityEventLogger class)
- **Status:** ‚úÖ FIXED in v2.0

### PF-046: Sensitive Fields Exposed in API Responses
- **Date:** 2026-02-25
- **Agent:** Security Auditor
- **Task:** M-003 SoftFactory security audit
- **Pitfall:** Login/register responses included fields that should never be exposed: `password_hash`, `is_locked`, `locked_until`. Allows attackers to detect locked accounts or weak security measures.
- **Prevention:** Sanitize API responses using `sanitize_login_response()` which removes all sensitive security fields before returning JSON. Only expose: id, email, name, role, created_at.
- **Files Created:** `backend/security_middleware.py` (sanitize_login_response function)
- **Files Fixed:** `backend/auth.py` (lines 196-197, sanitize before returning user object)
- **Status:** ‚úÖ FIXED in v2.0

---

## CI/CD & DevOps

### PF-031: Pre-commit Hooks Blocking Valid Commits
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** GitHub Actions + pre-commit setup
- **Pitfall:** Pre-commit hooks (black, isort, flake8, mypy) formatted code but didn't re-stage changes. User commits formatted code, pre-commit reformats it again, commit blocked. User confused, thinks they passed linting.
- **Prevention:** Auto-format tools (black, isort) must run with `--fix` or equivalent, AND automatic re-stage in pre-commit config. Use `stages: [commit]` for formatting, `stages: [push]` for slow checks (mypy). Add `.pre-commit-config.yaml` with `pass_filenames: true` for black/isort.
- **Reference:** `.pre-commit-config.yaml` ‚Äî black and isort run in commit stage with auto-fix enabled

### PF-032: GitHub Actions Matrix Testing Hidden Failures
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Multi-version test setup (Python 3.9/3.10/3.11)
- **Pitfall:** Test workflow runs matrix: one job fails (py3.9) but others pass (py3.10, py3.11). GitHub shows overall status GREEN because aggregate status only shows "1 failed, 2 passed". PR is mergeable despite failure.
- **Prevention:** Set `fail-fast: true` in matrix to stop on first failure. OR set branch protection rule: "Require all checks to pass" (checks ALL matrix combinations, not just aggregate). OR review detailed run logs before merging.
- **Reference:** `.github/workflows/test.yml` line 20: `fail-fast: false` (intentional for coverage visibility) ‚Äî requires manual review of py3.9 failures

### PF-033: Codecov Coverage Report Missing When Tests Don't Run
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** GitHub Actions coverage reporting
- **Pitfall:** Coverage upload to Codecov fails silently if no coverage.xml file generated. Workflow shows green, but Codecov never updates. User assumes coverage is tracked; weeks later discovers no history.
- **Prevention:** (1) Always verify coverage generation locally: `pytest tests/ --cov=backend --cov-report=xml` produces `coverage.xml`. (2) Add `fail_ci_if_error: true` to codecov-action to fail workflow if upload fails. (3) Log upload output: `verbose: true`.
- **Reference:** `.github/workflows/test.yml` line 82: `fail_ci_if_error: false` (currently soft-fail) ‚Äî recommend changing to `true` for production

### PF-034: Semantic Release Version Bumps Without Triggering Deployment
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Semantic versioning + release workflow
- **Pitfall:** Semantic-release creates v1.0.1 tag, BUT deploy workflow only triggers on `v*` tags created via `git push origin tag`. Since semantic-release creates tag via GitHub API, deploy workflow never runs (missing webhook trigger).
- **Prevention:** (1) semantic-release.yml must use `persistCredentials: true` and push tags with Git credentials. (2) Alternative: Use `workflow_dispatch` trigger in deploy.yml for manual release deployment. (3) Verify: After semantic-release runs, check GitHub releases page for new tag AND verify deploy workflow was triggered (Actions tab).
- **Reference:** `.github/workflows/release.yml` ‚Äî semantic-release step must push tags; deploy.yml watches for `push: tags: ['v*']`

### PF-035: Docker Image Build Cache Misses on Large .dockerignore
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Docker build optimization in build.yml
- **Pitfall:** .dockerignore excludes 70+ patterns but doesn't exclude `tests/` directory. 50MB of test files copied in Docker build layer, invalidating cache. Builds take 8 minutes instead of 2 minutes.
- **Prevention:** Review .dockerignore: exclude all development files (tests/, scripts/, docs/, *.md, .git/, .venv/, node_modules/). Keep only application code. Test locally: `docker build --progress=plain | grep "COPY . ."` ‚Äî should copy only backend/, web/, configs.
- **Reference:** `.dockerignore` ‚Äî currently excludes tests/ (good); verify before `docker build`

### PF-036: Security Scan False Positives Block Merges Unnecessarily
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** OWASP/CodeQL/Bandit integration
- **Pitfall:** Bandit reports "hardcoded SQL credentials" on test fixtures: `db_config = {'password': 'test'}` (intentionally hardcoded for testing). CodeQL flags `eval()` in code comment example. Merge blocked despite false positives.
- **Prevention:** (1) Suppress known false positives with `# nosec` comments (Bandit) or `# noqa: S101` (specific rule). (2) Document suppression: explain why it's safe. (3) Set `continue-on-error: true` for info-only scans; `fail` only for critical findings. (4) Review alerts weekly to catch real issues.
- **Reference:** `.github/workflows/security.yml` ‚Äî Bandit/Semgrep use `continue-on-error: true`; CodeQL is blocking (intentional)

### PF-037: Pre-commit Hook Performance Degrades Over Time
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Pre-commit maintenance
- **Pitfall:** Pre-commit runs on commit take 2 seconds (normal) ‚Üí 15 seconds ‚Üí 45 seconds over 3 weeks. User starts bypassing hooks (`--no-verify`). Root cause: pre-commit cache grows, or tool dependencies are mismatched.
- **Prevention:** (1) Periodically clean pre-commit cache: `pre-commit clean`. (2) Monitor hook performance: `pre-commit run --all-files` and time it. (3) Profile slow hooks: `pre-commit run mypy --all-files` (usually the slowest). (4) Consider marking slow checks with `stages: [push]` to skip on commit.
- **Workaround:** If a hook consistently slow (mypy > 10s), move to `stages: [push]` in `.pre-commit-config.yaml`

### PF-038: Commit Message Linting Against Merged PRs Fails
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Commitlint + GitHub Actions integration
- **Pitfall:** PR merged via GitHub "Squash and merge" button creates commit message like "PR #123: Description (automated)". Commitlint expects "feat: description" format. Deploy workflow rejects the commit before deploying.
- **Prevention:** (1) Enforce conventional commits in GitHub PR template (tell users what format to use). (2) Configure commitlint to accept auto-generated messages: add `feat(pr|merge|squash): .*` to allowed types. (3) Educate team: always write conventional-format commit messages in PR titles (GitHub auto-uses as commit message).
- **Reference:** `.commitlintrc.json` ‚Äî currently enforces strict format; may need relaxation for GitHub auto-merges

### PF-039: Workflow Secrets Not Available in Pull Requests from Forks
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Security scanning for PRs
- **Pitfall:** External contributor opens PR from fork. Security scan workflow (CodeQL, Trivy) skipped silently because `secrets.GITHUB_TOKEN` not available in fork context. Malicious code merged without scanning.
- **Prevention:** (1) Use `if: github.event_name != 'pull_request_target'` to skip sensitive steps on forks. (2) OR use read-only secrets in PR context: CodeQL allowed, but deployment denied. (3) Always review fork PRs manually before merge. (4) GitHub default: secrets unavailable on forks (safe behavior), but some tools fail silently.
- **Reference:** `.github/workflows/security.yml` and `.github/workflows/deploy.yml` both skip on fork PRs

### PF-040: Database Migrations in CI Not Validated Against Production Schema
- **Date:** 2026-02-25
- **Agent:** CI/CD Pipeline Agent
- **Task:** Test database setup (.github/workflows/test.yml)
- **Pitfall:** CI uses fresh PostgreSQL 15 container. Migration script works in CI but fails in production (PostgreSQL 13, existing schema). Migration adds column with DEFAULT constraint that's incompatible with old version.
- **Prevention:** (1) Match CI database version to production version. (2) Test migrations against production DB snapshot (sanitized). (3) Always test rollback: `migrate up; migrate down; migrate up`. (4) Document minimum version support: "PostgreSQL 13+".
- **Reference:** `.github/workflows/test.yml` line 25: `postgres:15` ‚Äî should match production version


---

## PF-008: ngrok Tunnel Issues (2026-02-25)

**Category:** Infrastructure / Networking
**Severity:** HIGH
**Discovered:** Network Access Agent
**Status:** DOCUMENTED

### Problem
ngrok tunnels can disconnect unexpectedly due to:
- Network interruption (WiFi drop, VPN disconnect)
- ngrok API rate limits (free tier: 20 tunnels/40h)
- Auth token expiration or revocation
- Firewall/proxy interference
- Local port conflicts

### Prevention
1. **Use monitoring mode:** `bash scripts/ngrok-start.sh --monitor`
   - Enables auto-reconnect (max 5 attempts, 10s intervals)
   - Health checks every 30 seconds
   - Fallback to localtunnel if ngrok fails

2. **Set auth token:** Reduces API rate limits and prevents URL hijacking
   - Get token: https://dashboard.ngrok.com/get-started/your-authtoken
   - Set via: `export NGROK_AUTHTOKEN="..."`

3. **Check prerequisites:**
   - Flask app running on port 8000: `curl http://localhost:8000/health`
   - ngrok in PATH: `ngrok version`
   - Firewall allows outbound HTTPS (port 443)

4. **Monitor tunnel status:**
   - Check ngrok web inspector: http://127.0.0.1:4040
   - Query ngrok API: `curl http://127.0.0.1:4041/api/tunnels | jq`
   - Check log file: `tail logs/ngrok.log`

### Recovery
```bash
# Immediate: Stop and restart
bash scripts/ngrok-start.sh --monitor

# Fallback: Use localtunnel
bash scripts/localtunnel-start.sh

# Investigate: Check logs
tail -50 logs/ngrok.log | grep -i "error\|disconnect"
```

### Reference
- Prevention: `docs/PUBLIC_ACCESS_GUIDE.md` Section 3-4
- Troubleshooting: `docs/PUBLIC_ACCESS_GUIDE.md` Section 10

---

## PF-009: Access Logs Disk Usage (2026-02-25)

**Category:** Operations / Storage
**Severity:** MEDIUM
**Discovered:** Network Access Agent
**Status:** DOCUMENTED

### Problem
Access logs grow continuously:
- ~1KB per request (JSON log entry)
- 10K requests/day = 10MB/day
- 1 year of logs = 3.65GB

Without cleanup, disk fills up and logging stops.

### Prevention
1. **Implement log rotation:**
   ```bash
   # Delete logs older than 30 days
   find logs/ -name "access*.log*" -mtime +30 -delete
   
   # Archive by month
   tar -czf logs/archive-$(date +%Y%m).tar.gz logs/access*.log
   ```

2. **Set cron job:**
   ```bash
   # Add to crontab (runs daily at 2 AM)
   0 2 * * * cd /home/user/Project && bash scripts/cleanup-logs.sh
   ```

3. **Monitor disk usage:**
   ```bash
   # Check log directory size
   du -sh logs/
   
   # Alert if > 1GB
   if [ $(du -s logs | cut -f1) -gt 1048576 ]; then
     # Send alert
   fi
   ```

### Recovery
```bash
# Delete old logs immediately
find logs/ -name "*.log" -mtime +7 -delete

# Compress metrics to free space
gzip logs/metrics.json

# Archive and delete incident logs
tar -czf logs/incidents-archive.tar.gz logs/incidents.jsonl
rm logs/incidents.jsonl
```

### Reference
- Cleanup script: `scripts/cleanup-logs.sh` (create if needed)
- Monitoring: `docs/PUBLIC_ACCESS_GUIDE.md` Section 7

---

## PF-010: CORS Whitelist Out of Sync (2026-02-25)

**Category:** Integration / Configuration
**Severity:** MEDIUM
**Discovered:** Network Access Agent
**Status:** DOCUMENTED

### Problem
When ngrok URL changes:
- Old URL in CORS whitelist causes issues
- New URL not in whitelist = CORS errors
- Users get "Cross-Origin Request Blocked"

### Prevention
1. **Dynamic CORS configuration:**
   - Don't hardcode ngrok URLs
   - Fetch dynamically from `get_public_url()`
   - See: `docs/PUBLIC_ACCESS_INTEGRATION.md` Step 3

2. **Auto-update CORS:**
   - Run `bash scripts/update-public-urls.sh` after tunnel starts
   - Script notifies Flask app of new URL
   - Flask updates CORS allowlist automatically

3. **Test CORS after restart:**
   ```bash
   # After starting ngrok
   PUBLIC_URL=$(cat logs/ngrok-url.txt)
   curl -H "Origin: $PUBLIC_URL" \
     -H "Access-Control-Request-Method: POST" \
     http://localhost:8000/api/health
   ```

### Recovery
```bash
# Immediate: Restart Flask app
python start_platform.py

# Or manually update CORS
curl -X POST http://localhost:8000/api/admin/public-url-update \
  -d '{"url": "'$PUBLIC_URL'", "type": "ngrok"}'
```

### Reference
- Integration guide: `docs/PUBLIC_ACCESS_INTEGRATION.md` Step 3
- Update script: `scripts/update-public-urls.sh`

---

## Error Tracking & Observability

### PF-041: Pattern Detection Requires Multiple Error Occurrences
- **Date:** 2026-02-25
- **Agent:** 09-Development Lead (Error Tracking Task #4)
- **Task:** Error Tracking System Implementation
- **Pitfall:** Error tracking system only flags as "pattern" when same error type/message occurs 2+ times. Single isolated errors never get prevention suggestions, making rare bugs invisible for long periods.
- **Prevention:** Log all errors to tracking system, even single occurrences. Periodic batch analysis (daily) can identify rare patterns. For production: daily pattern report shows "1-occurrence errors" separately for monitoring.
- **Files Created:** `backend/error_tracker.py` (ErrorTracker class), `backend/error_api.py` (6 API endpoints)
- **Implementation Note:** Pattern threshold set to 2+ occurrences intentionally; adjust `PatternDetector.detect_patterns()` for different sensitivity.

### PF-042: Error Severity Calculation Based on Frequency Alone
- **Date:** 2026-02-25
- **Agent:** 09-Development Lead (Error Tracking Task #4)
- **Task:** Error Tracking System Implementation
- **Pitfall:** Error severity calculated only by frequency and error type. An "AttributeError: 'NoneType' has no attribute 'id'" occurring 100 times still scored as "medium" even though it's likely a critical issue affecting all users.
- **Prevention:** Extend PreventionEngine._calculate_severity() to analyze error message content (database, auth, API-related keywords get higher severity). Correlate with business impact: errors affecting payment/auth always "high" or "critical" regardless of frequency.
- **Files Created:** `backend/error_tracker.py` (PreventionEngine._calculate_severity method)
- **Enhancement Opportunity:** Add machine learning pattern analysis; train on historical production errors.

### PF-043: Prevention Suggestions Without Code Examples Not Actionable
- **Date:** 2026-02-25
- **Agent:** 09-Development Lead (Error Tracking Task #4)
- **Task:** Error Tracking System Implementation
- **Pitfall:** Generic prevention rules like "Add input validation" are not immediately actionable. Developers must search for examples elsewhere, delaying fixes.
- **Prevention:** Every PreventionEngine.suggest_fix() must include concrete code example matching the error type. Developers copy/paste example, adapting to their code context.
- **Files Created:** `backend/error_tracker.py` (PreventionEngine._generate_code_example method with AttributeError, KeyError, TypeError examples)
- **Status:** ‚úÖ Implemented with 4 code examples; can be extended

### PF-044: Error Tracking System Cache Only ‚Äî No Persistence Across Restarts
- **Date:** 2026-02-25
- **Agent:** 09-Development Lead (Error Tracking Task #4)
- **Task:** Error Tracking System Implementation
- **Pitfall:** Current ErrorTracker implementation stores errors in memory dict (error_cache). On Flask restart, all error history lost. Patterns detected over days/weeks cannot be retrieved after deployment.
- **Prevention:** (Phase 2 enhancement) Implement `ErrorLog` and `ErrorPattern` SQLAlchemy models. On startup, query recent errors from DB and rebuild pattern cache. ErrorTracker.log_error() persists to database for long-term analysis.
- **Current Limitation:** MVP level; suitable for monitoring within single session. Not production-ready for multi-day pattern analysis.
- **Upgrade Path:** `backend/models.py` needs ErrorLog model; `backend/error_tracker.py` needs database persistence layer

### PF-045: Pitfalls Registry Auto-Update Not Fully Integrated
- **Date:** 2026-02-25
- **Agent:** 09-Development Lead (Error Tracking Task #4)
- **Task:** Error Tracking System Implementation
- **Pitfall:** Task requirement specified: "When pattern marked resolved, auto-append to pitfalls.md". Current implementation has `report_pattern_fixed()` method but no integration with `shared-intelligence/pitfalls.md` file writing.
- **Prevention:** Add ErrorTracker.auto_update_pitfalls(pattern) method that: (1) Detects pattern severity and type, (2) Generates pitfall entry markdown, (3) Appends to pitfalls.md with proper formatting. Call after pattern resolution approved.
- **Current Status:** ‚ö†Ô∏è Partially implemented ‚Äî infrastructure ready, auto-write logic deferred to Phase 2
- **Reference:** Mission requirement lines: "Create auto-update mechanism: ErrorPattern ‚Üí pitfall entry format"

### PF-046: OAuth Provider Import from Relative Module Path
- **Date:** 2026-02-26
- **Agent:** Team A (SNS OAuth Implementation)
- **Task:** Task #14 ‚Äî SNS OAuth & Social Login Implementation
- **Pitfall:** Initial import used `from ..models import SNSOAuthState` (relative path up two levels) which failed. Correct path from `backend/auth.py` is `from .models import SNSOAuthState` (same package).
- **Prevention:** When importing from within Flask blueprint packages, use single-dot relative imports for same-level modules. Test imports with: `python3 -c "from backend.auth import auth_bp"` before deployment.
- **Files:** `backend/auth.py` line 232

### PF-047: OAuth User Model Missing Fields in to_dict()
- **Date:** 2026-02-26
- **Agent:** Team A (SNS OAuth Implementation)
- **Task:** Task #14 ‚Äî SNS OAuth & Social Login Implementation
- **Pitfall:** User.to_dict() did not include oauth_provider, oauth_id fields added to User model. This caused OAuth users to appear as non-social-login users after token refresh.
- **Prevention:** When adding new columns to SQLAlchemy models, immediately update to_dict() method to include all user-facing fields. Add to model documentation template.
- **Files:** `backend/models.py` User.to_dict() method