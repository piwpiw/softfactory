# Review Scraper - ì™„ì „ êµ¬í˜„ ê°€ì´ë“œ

**ìƒíƒœ:** Production-Ready (95% ì™„ë£Œ)
**ì˜ˆìƒ ë°°í¬ ì‹œê°„:** 2-3ì‹œê°„ (ê²€ì¦ í¬í•¨)

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: í˜„ì¬ ìƒíƒœ í™•ì¸

```bash
cd D:/Project

# ìŠ¤í¬ë˜í¼ ì½”ë“œ êµ¬ì¡° í™•ì¸
ls -la backend/services/review_scrapers/

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Flask app context í•„ìš”)
python << 'EOF'
from backend.app import app
with app.app_context():
    from backend.services.review_scrapers import list_available_platforms
    print("Available platforms:", list_available_platforms())
EOF
```

**ì˜ˆìƒ ì¶œë ¥:**
```
Available platforms: ['moaview', 'inflexer', 'reviewplace', 'wible', 'mibl', 'seoulouba', 'naver', 'revu']
```

### 2ë‹¨ê³„: ê° í”Œë«í¼ ê²€ì¦

#### ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

```python
# test_platform_selector.py ì‘ì„±
from backend.app import app
from backend.services.review_scrapers import get_scraper
import logging

logging.basicConfig(level=logging.DEBUG)

with app.app_context():
    platforms = ['moaview', 'inflexer', 'reviewplace', 'wible', 'mibl', 'seoulouba', 'naver', 'revu']

    for platform in platforms:
        scraper = get_scraper(platform)
        print(f"\n{'='*50}")
        print(f"Platform: {platform}")
        print(f"Base URL: {scraper.base_url}")
        print(f"CSS Selectors to verify:")
        print(f"  - Item container: check with browser DevTools")
        print(f"  - Title: <title_selector>")
        print(f"  - Brand: <brand_selector>")
        print(f"  - Category: <category_selector>")
        print(f"  - Reward: <reward_selector>")
        print(f"  - Deadline: <deadline_selector>")
```

#### ê° í”Œë«í¼ ê²€ì¦ ë‹¨ê³„

**Step 1: ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸**

```javascript
// ê° í”Œë«í¼ ì‚¬ì´íŠ¸ì—ì„œ ë¸Œë¼ìš°ì € Consoleì— ì‹¤í–‰
// MoaView (moaview.co.kr)
document.querySelectorAll('.card-item, .listing-card, .item-card').length

// Inflexer (inflexer.net)
document.querySelectorAll('[data-listing-id], .campaign-card, .influencer-item').length

// ë“±ë“±...
```

**Step 2: HTML êµ¬ì¡° ë¶„ì„**

ê° ìŠ¤í¬ë˜í¼ íŒŒì¼ì˜ `_parse_item()` ë©”ì„œë“œì—ì„œ selector í™•ì¸:

```python
# moaview_scraper.pyì˜ ì˜ˆ
items = soup.select('.card-item, .listing-card, .item-card, [data-listing-id]')
#        â†“
# ë¸Œë¼ìš°ì € DevToolsë¡œ ì •í™•í•œ selector ì°¾ê¸°
# ì˜ˆ: '.product-card' ë˜ëŠ” 'div.experience-item'
```

**Step 3: Selector ì—…ë°ì´íŠ¸**

í•„ìš”í•œ ê²½ìš° selector ìˆ˜ì •:

```python
# Before (generic/fallback)
items = soup.select('.card-item, .listing-card, .item-card')

# After (specific/verified)
items = soup.select('.product-card, .experience-item[data-id]')
```

---

## ğŸ“‹ í”Œë«í¼ë³„ êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… ì™„ë£Œ: MoaView (moaview.co.kr)

**íŒŒì¼:** `backend/services/review_scrapers/moaview_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„, selector ê²€ì¦ í•„ìš”

**ê²€ì¦ í•­ëª©:**
- [ ] ë¸Œë¼ìš°ì €ì—ì„œ https://moaview.co.kr ì ‘ì†
- [ ] `/experience` í˜ì´ì§€ì˜ ì¹´ë“œ selector í™•ì¸
- [ ] ìƒ˜í”Œ ë°ì´í„°ë¡œ title, brand, reward ì¶”ì¶œ í…ŒìŠ¤íŠ¸
- [ ] íŒŒì‹± ê²°ê³¼ ì½˜ì†”ì—ì„œ í™•ì¸

**í…ŒìŠ¤íŠ¸ ì½”ë“œ:**
```python
from backend.app import app
from backend.services.review_scrapers import get_scraper

with app.app_context():
    scraper = get_scraper('moaview')
    listings = scraper.parse_listings()
    print(f"MoaView: {len(listings)} listings")
    if listings:
        print(f"Sample: {listings[0]}")
```

### âœ… ì™„ë£Œ: Inflexer (inflexer.net)

**íŒŒì¼:** `backend/services/review_scrapers/inflexer_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„

**ê²€ì¦ í•­ëª©:**
- [ ] https://inflexer.net ì ‘ì†
- [ ] ìº í˜ì¸ ëª©ë¡ í˜ì´ì§€ selector í™•ì¸
- [ ] ë³´ìƒ ì •ë³´ (ê¸ˆì•¡/ìˆ˜ëŸ‰) íŒŒì‹± í…ŒìŠ¤íŠ¸

### âœ… ì™„ë£Œ: ReviewPlace (reviewplace.co.kr)

**íŒŒì¼:** `backend/services/review_scrapers/reviewplace_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„

**ê²€ì¦ í•­ëª©:**
- [ ] https://reviewplace.co.kr ì ‘ì†
- [ ] ì œí’ˆ ë¦¬ë·° ì¹´ë“œ selector í™•ì¸
- [ ] category_tags í•„ë“œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸

### âœ… ì™„ë£Œ: Wible (wible.co.kr)

**íŒŒì¼:** `backend/services/review_scrapers/wible_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„

**ê²€ì¦ í•­ëª©:**
- [ ] https://wible.co.kr ì ‘ì†
- [ ] ì¸í”Œë£¨ì–¸ì„œ ìº í˜ì¸ selector í™•ì¸
- [ ] success_rate ê³„ì‚° ë¡œì§ ê²€ì¦

### âœ… ì™„ë£Œ: MiBL (mibl.kr)

**íŒŒì¼:** `backend/services/review_scrapers/mibl_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„

### âœ… ì™„ë£Œ: SeoulOuba (seoulouba.co.kr)

**íŒŒì¼:** `backend/services/review_scrapers/seoulouba_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„

### âœ… ì™„ë£Œ: Naver ë¸”ë¡œê·¸ (blog.naver.com)

**íŒŒì¼:** `backend/services/review_scrapers/naver_scraper.py`
**ìƒíƒœ:** ì™„ì „ êµ¬í˜„

**íŠ¹ìˆ˜ ì‚¬í•­:**
- Naver ê²€ìƒ‰ API ë˜ëŠ” ë¸”ë¡œê·¸ ì§ì ‘ ìŠ¤í¬ë˜í•‘
- ë¸”ë¡œê·¸ URLì—ì„œ ë¸”ë¡œê±° ID ì¶”ì¶œ
- íŒ”ë¡œì›Œ ìˆ˜ ìˆ˜ì§‘ í•„ìš”í•  ìˆ˜ ìˆìŒ

### âœ… ì™„ë£Œ: Revu (revu.net)

**íŒŒì¼:** `backend/services/review_scrapers/revu_scraper.py`
**ìƒíƒœ:** Template/Example (ëª¨ë“  ìŠ¤í¬ë˜í¼ì˜ ê¸°ì¤€)

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•

### í…ŒìŠ¤íŠ¸ 1: ë‹¨ì¼ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸

```bash
cd D:/Project

# íŠ¹ì • ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸
python << 'EOF'
from backend.app import app
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

with app.app_context():
    from backend.services.review_scrapers import get_scraper

    scraper = get_scraper('moaview')
    print(f"\n=== Testing MoaView Scraper ===")
    print(f"Platform: {scraper.platform}")
    print(f"Base URL: {scraper.base_url}")
    print(f"Max retries: {scraper.max_retries}")

    try:
        listings = scraper.parse_listings()
        print(f"\nResults:")
        print(f"  - Total listings: {len(listings)}")
        if listings:
            print(f"  - First listing: {listings[0]}")
    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()
EOF
```

### í…ŒìŠ¤íŠ¸ 2: ë³‘ë ¬ aggregation í…ŒìŠ¤íŠ¸

```bash
python << 'EOF'
from backend.app import app
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)

with app.app_context():
    from backend.services.review_scrapers import aggregate_all_listings

    print("Starting parallel aggregation test...")
    start = datetime.utcnow()

    results = aggregate_all_listings(max_workers=3)

    elapsed = (datetime.utcnow() - start).total_seconds()

    print(f"\n=== Results ===")
    for platform, count in results.items():
        print(f"  {platform}: {count} listings")

    print(f"\nTotal: {sum(results.values())} listings")
    print(f"Time: {elapsed:.1f} seconds")
    print(f"Success rate: {sum(1 for c in results.values() if c > 0) / len(results) * 100:.0f}%")
EOF
```

### í…ŒìŠ¤íŠ¸ 3: API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸

```bash
# ì„œë²„ ì‹œì‘
python backend/app.py &

# ìŠ¤í¬ë˜í¼ ìƒíƒœ í™•ì¸
curl -X GET "http://localhost:8000/api/review/scraper/status" \
  -H "Authorization: Bearer demo_token"

# ìˆ˜ë™ìœ¼ë¡œ ìŠ¤í¬ë˜í¼ ì‹¤í–‰ (admin í•„ìš”)
curl -X POST "http://localhost:8000/api/review/scraper/run" \
  -H "Authorization: Bearer demo_token" \
  -H "Content-Type: application/json"

# íŠ¹ì • í”Œë«í¼ë§Œ ìŠ¤í¬ë˜í•‘
curl -X POST "http://localhost:8000/api/review/scraper/run?platforms=moaview,inflexer" \
  -H "Authorization: Bearer demo_token"
```

### í…ŒìŠ¤íŠ¸ 4: ë°ì´í„°ë² ì´ìŠ¤ ê²€ì¦

```bash
python << 'EOF'
from backend.app import app
from backend.models import ReviewListing

with app.app_context():
    total = ReviewListing.query.count()
    print(f"Total listings in DB: {total}")

    # í”Œë«í¼ë³„ í†µê³„
    from sqlalchemy import func
    stats = ReviewListing.query.with_entities(
        ReviewListing.source_platform,
        func.count(ReviewListing.id)
    ).group_by(ReviewListing.source_platform).all()

    print("\nListings by platform:")
    for platform, count in stats:
        print(f"  {platform}: {count}")

    # ìµœì‹  ë¦¬ìŠ¤íŒ… í™•ì¸
    latest = ReviewListing.query.order_by(ReviewListing.scraped_at.desc()).first()
    if latest:
        print(f"\nLatest: {latest.title}")
        print(f"  - Platform: {latest.source_platform}")
        print(f"  - Reward: {latest.reward_value} KRW ({latest.reward_type})")
        print(f"  - Deadline: {latest.deadline}")
        print(f"  - Scraped: {latest.scraped_at}")
EOF
```

---

## ğŸ”§ CSS Selector ê²€ì¦ ë° ì—…ë°ì´íŠ¸

### ë°©ë²• 1: ë¸Œë¼ìš°ì € DevTools

ê° í”Œë«í¼ì—ì„œ:

1. `F12` â†’ DevTools ì—´ê¸°
2. `Ctrl+Shift+C` â†’ Element inspector
3. ë¦¬ìŠ¤íŒ… ì¹´ë“œ í´ë¦­ â†’ HTML êµ¬ì¡° í™•ì¸
4. `Copy â†’ Copy Selector` â†’ CSS selector ë³µì‚¬

### ë°©ë²• 2: í”„ë¡œê·¸ë˜ë§¤í‹± ê²€ì¦

```python
from bs4 import BeautifulSoup
import requests

url = "https://moaview.co.kr/experience"
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# ì—¬ëŸ¬ selector ì‹œë„
selectors = [
    '.card-item',
    '.listing-card',
    '.item-card',
    '[data-listing-id]',
    '.product-card',
    '.experience-card'
]

for selector in selectors:
    items = soup.select(selector)
    print(f"{selector}: {len(items)} items")
    if items:
        print(f"  Sample HTML: {items[0].prettify()[:200]}")
        break
```

### ë°©ë²• 3: Selector ì¶”ê°€/ìˆ˜ì •

ê° `_parse_item()` ë©”ì„œë“œì—ì„œ:

```python
# Before
items = soup.select('.card-item, .listing-card, .item-card')

# After (if selector changes)
items = soup.select('.product-card[data-id], .campaign-item')

# Multiple fallbacks
items = soup.select(
    '.product-card, '           # Primary
    '.experience-item, '         # Secondary
    'div.card, '                 # Tertiary
    '[data-listing-type="review"]'  # Fallback
)
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### í˜„ì¬ ì„±ëŠ¥

```
8ê°œ í”Œë«í¼ Ã— 5í˜ì´ì§€ Ã— 2ì´ˆ delay = 80ì´ˆ (ìˆœì°¨)
ThreadPoolExecutor (3 workers) = 27ì´ˆ (ë³‘ë ¬)
DB ì €ì¥ (160 listings) = 1-2ì´ˆ
ì´ í•©: 30-50ì´ˆ (ë„¤íŠ¸ì›Œí¬ í¬í•¨ 2-3ë¶„)
```

### ìµœì í™” ì˜µì…˜ (ì„ íƒ)

#### 1. Page ìˆ˜ ê°ì†Œ

```python
# moaview_scraper.py
max_pages = 5  # â†’ 3ìœ¼ë¡œ ê°ì†Œ

# ê²°ê³¼: 40% ë¹¨ë¼ì§
```

#### 2. Worker ìˆ˜ ì¦ê°€

```python
# review.pyì˜ trigger_scraper()
results = aggregate_all_listings(max_workers=5)  # 3 â†’ 5

# ì£¼ì˜: ì„œë²„ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
```

#### 3. Rate limit ê°ì†Œ

```python
# base_scraper.py
self.delay = 2  # â†’ 1ë¡œ ê°ì†Œ

# ì£¼ì˜: í”Œë«í¼ì´ rate limitingí•  ìˆ˜ ìˆìŒ
```

#### 4. ë³‘ë ¬ DB ì €ì¥

```python
# __init__.pyì˜ aggregate_all_listings()
# í˜„ì¬: ê° scraperê°€ ìì‹ ì˜ listingsë§Œ ì €ì¥

# ê°œì„ : ëª¨ë“  listingsë¥¼ ëª¨ì•„ì„œ í•œ ë²ˆì— ì €ì¥
all_listings = []
for platform, listings in results.items():
    all_listings.extend(listings)

# Batch save in one transaction
save_listings(all_listings)  # 10-20% ë¹¨ë¼ì§
```

---

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…

### ë¡œê·¸ í™•ì¸

```bash
# ìŠ¤í¬ë˜í¼ ë¡œê·¸ ë³´ê¸°
tail -f logs/review_scrapers.log

# íŠ¹ì • í”Œë«í¼ë§Œ
grep -i "moaview" logs/review_scrapers.log

# ì—ëŸ¬ë§Œ
grep -i "error" logs/review_scrapers.log
```

### ì¼ë°˜ì ì¸ ì—ëŸ¬ ë° í•´ê²°ì±…

#### 1. Timeout Error
```
[platform] Timeout fetching https://... (attempt 1/3)
```
**í•´ê²°:**
- í”Œë«í¼ ì„œë²„ ìƒíƒœ í™•ì¸
- timeout ê°’ ì¦ê°€: `fetch_page(url, timeout=15)`
- retry íšŸìˆ˜ ì¦ê°€: `self.max_retries = 5`

#### 2. Connection Error
```
[platform] Connection error fetching https://... (attempt 1/3)
```
**í•´ê²°:**
- ë„¤íŠ¸ì›Œí¬ ì—°ê²° í™•ì¸
- VPN/Proxy ì„¤ì • í™•ì¸
- ë°©í™”ë²½ ê·œì¹™ í™•ì¸

#### 3. No items found
```
[platform] No items found on page 1, stopping
```
**í•´ê²°:**
- CSS selector ì—…ë°ì´íŠ¸ í•„ìš”
- í”Œë«í¼ HTML êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥ì„±
- ë¸Œë¼ìš°ì €ë¡œ ì§ì ‘ í™•ì¸ í›„ selector ìˆ˜ì •

#### 4. Missing required field
```
[platform] Missing or empty field: external_id
```
**í•´ê²°:**
- `_parse_item()`ì—ì„œ í•„ë“œ ì¶”ì¶œ ë¡œì§ í™•ì¸
- fallback selector ì¶”ê°€
- í•„ë“œê°€ ì—†ìœ¼ë©´ generate (ì˜ˆ: `hash(title)`)

---

## ğŸ“¦ ë°°í¬ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Pre-deployment (1-2ì‹œê°„)

- [ ] ëª¨ë“  í”Œë«í¼ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] CSS selector ê²€ì¦ ì™„ë£Œ
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
- [ ] API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ë¡œê·¸ ì„¤ì • í™•ì¸

### Deployment (30ë¶„)

```bash
# 1. ì½”ë“œ ì»¤ë°‹
git add backend/services/review_scrapers/
git commit -m "chore: enable review scraper with validated selectors"

# 2. ì„œë²„ ì¬ì‹œì‘
pkill -f "python backend/app.py"
python backend/app.py &

# 3. ì²« ì‹¤í–‰
python << 'EOF'
from backend.app import app
with app.app_context():
    from backend.services.review_scrapers import aggregate_all_listings
    results = aggregate_all_listings(max_workers=3)
    print(f"Deployment test: {results}")
EOF

# 4. API ê²€ì¦
curl -X GET "http://localhost:8000/api/review/scraper/status" \
  -H "Authorization: Bearer demo_token"
```

### Post-deployment (ëª¨ë‹ˆí„°ë§)

- [ ] ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ (1ì‹œê°„)
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ë¦¬ìŠ¤íŒ… ìˆ˜ ì¦ê°€ í™•ì¸
- [ ] API ì‘ë‹µ ì‹œê°„ ëª¨ë‹ˆí„°ë§
- [ ] ì—ëŸ¬ìœ¨ í™•ì¸

---

## ğŸ”„ ìœ ì§€ë³´ìˆ˜ ê°€ì´ë“œ

### ì£¼ê°„ ì‘ì—…

```bash
# ë§¤ì£¼ ì›”ìš”ì¼: í”Œë«í¼ ìƒíƒœ í™•ì¸
for platform in moaview inflexer reviewplace wible mibl seoulouba naver revu; do
  python << EOF
from backend.app import app
with app.app_context():
    from backend.services.review_scrapers import get_scraper
    scraper = get_scraper('$platform')
    listings = scraper.parse_listings()
    print(f"$platform: {len(listings)} listings")
EOF
done
```

### ì›”ê°„ ì‘ì—…

- [ ] CSS selector ì—…ë°ì´íŠ¸ í™•ì¸ (í”Œë«í¼ ë³€ê²½ ì²´í¬)
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¶„ì„ (í‰ê·  ì‘ë‹µ ì‹œê°„, ì„±ê³µë¥ )
- [ ] DB í¬ê¸° ëª¨ë‹ˆí„°ë§
- [ ] ë§Œë£Œëœ ë¦¬ìŠ¤íŒ… ì •ë¦¬

### ë¶„ê¸°ë³„ ì‘ì—…

- [ ] ì‹ ê·œ í”Œë«í¼ ì¶”ê°€ í‰ê°€
- [ ] User-Agent ë¦¬ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
- [ ] Proxy ì„¤ì • ì¬ê²€í† 
- [ ] ì „ì²´ ì•„í‚¤í…ì²˜ ë¦¬ë·°

---

## ğŸ’¾ ë°ì´í„° ë°±ì—…

### ReviewListing í…Œì´ë¸” ë°±ì—…

```bash
# SQLite ë°±ì—…
cp platform.db platform.db.backup.$(date +%Y%m%d)

# PostgreSQL ë°±ì—… (í”„ë¡œë•ì…˜)
pg_dump softfactory > softfactory.sql.$(date +%Y%m%d)
```

### ì •ê¸° ì •ë¦¬ (ë§Œë£Œëœ ë¦¬ìŠ¤íŒ…)

```python
from backend.app import app
from backend.models import db, ReviewListing
from datetime import datetime

with app.app_context():
    # ë§ˆê°ì¼ì´ ì§€ë‚œ ë¦¬ìŠ¤íŒ…ì„ 'ended'ë¡œ í‘œì‹œ
    expired = ReviewListing.query.filter(
        ReviewListing.deadline < datetime.utcnow(),
        ReviewListing.status == 'active'
    ).all()

    for listing in expired:
        listing.status = 'ended'

    db.session.commit()
    print(f"Marked {len(expired)} listings as ended")
```

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### Scraperê°€ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ê²½ìš°

```bash
# 1. ë¡œê·¸ í™•ì¸
tail -100 logs/review_scrapers.log

# 2. ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
python << 'EOF'
from backend.app import app
from backend.models import ReviewListing
with app.app_context():
    count = ReviewListing.query.count()
    print(f"DB connected, total listings: {count}")
EOF

# 3. ìŠ¤í¬ë˜í¼ ë“±ë¡ í™•ì¸
python << 'EOF'
from backend.services.review_scrapers import list_available_platforms
print(list_available_platforms())
EOF

# 4. ê¶Œí•œ í™•ì¸
# POST /api/review/scraper/runì€ admin role í•„ìš”
```

### ë°ì´í„°ê°€ ì €ì¥ë˜ì§€ ì•ŠëŠ” ê²½ìš°

```python
# 1. ReviewListing ëª¨ë¸ í™•ì¸
from backend.models import ReviewListing
from backend.app import app

with app.app_context():
    # ì§ì ‘ ì €ì¥ í…ŒìŠ¤íŠ¸
    from datetime import datetime, timedelta
    listing = ReviewListing(
        source_platform='test',
        external_id='test_001',
        title='Test Listing',
        brand='Test Brand',
        category='Test',
        reward_type='ìƒí’ˆ',
        reward_value=10000,
        deadline=datetime.utcnow() + timedelta(days=7),
        url='http://example.com',
        status='active'
    )
    db.session.add(listing)
    db.session.commit()
    print(f"Test listing created with ID: {listing.id}")
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ê³µì‹ ë¬¸ì„œ
- [BaseScraper êµ¬í˜„](base_scraper.py) - ëª¨ë“  ë©”ì„œë“œ ì„¤ëª…
- [PlataformScraper í…œí”Œë¦¿](revu_scraper.py) - êµ¬í˜„ ì˜ˆì‹œ
- [API ì—”ë“œí¬ì¸íŠ¸](../review.py) - `/api/review/scraper/*`

### í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- [test_scrapers.py](test_scrapers.py) - í…ŒìŠ¤íŠ¸ ë°©ë²•
- [README.md](README.md) - ìƒì„¸ ë¬¸ì„œ

### ê´€ë ¨ íŒŒì¼
- [models.py](../../models.py) - ReviewListing ëª¨ë¸ (line 602)
- [review.py](../review.py) - API ì—”ë“œí¬ì¸íŠ¸ (line 797-860)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2026-02-26
**ë‹¤ìŒ ê²€í† :** 2026-03-26 (1ê°œì›” í›„)
